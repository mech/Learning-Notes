# Postgres

* [Postgres Guide](http://postgresguide.com/)
* [PG Exercises](https://pgexercises.com/)
* [Modern SQL](https://modern-sql.com/)

> It's preferable to exercise Postgres's ability to execute JOINs rather than play with your network latency.

RDBMS traditionally feature strong consistency and high availability at the expense of partition tolerance. They are optimized for writes.

NoSQL are optimized for reads with eventual consistency in mind. For MailLog/audit log/activity feed purposes, if we just write once and read many times, then choosing NoSQL is the correct approach.

```
How many relationships are in your data? 
What is the level of complexity in your data? 
How often do the data change? 
How often does your application query the data? 
How often does your application query the relationship underlying the data?
How often do your users update the data? 
How often do your users update the logic in the data?
How critical is your Application in a disaster scenario?
```

**https://wiki.postgresql.org/wiki/Don't_Do_This**

* [HOWTO](https://popsql.io/learn-sql/postgresql/)
* [The Internals of PostgreSQL](http://www.interdb.jp/pg/)
* [PostgreSQL Exercises](https://pgexercises.com/)
* [Our Postgres Infrastructure](http://blog.honeybadger.io/our-postgres-infrastructure/)
* [Introduction to PostgreSQL physical storage](http://rachbelaid.com/introduction-to-postgres-physical-storage/)
* [Chapter 66. Database Physical Storage](https://www.postgresql.org/docs/10/static/storage.html)
* [Postgres at Any Scale](https://www.youtube.com/watch?v=_wU2dglywAU)
* [HypoPG - hypothetical indexes](https://github.com/dalibo/hypopg)
* [5 Things I wish my grandfather told me about ActiveRecord and Postgres](https://medium.com/carwow-product-engineering/5-things-i-wish-my-grandfather-told-me-about-activerecord-and-postgres-93416faa09e7)
* [My Top 10 Postgres Features and Tips for 2016](http://www.craigkerstiens.com/2015/12/29/my-postgres-top-10-for-2016/)
* [pg_cron: Run periodic jobs in PostgreSQL](https://www.citusdata.com/blog/2016/09/09/pgcron-run-periodic-jobs-in-postgres/)
* [Performance Tuning Queries in PostgreSQL](https://www.geekytidbits.com/performance-tuning-postgres/)
* [Debugging complex PostgreSQL queries with **pgdebug**](http://korban.net/posts/postgres/2017-09-18-debugging-complex-postgres-queries-with-pgdebug/)
* [The case against ORMs](http://korban.net/posts/postgres/2017-11-02-the-case-against-orms/)
* [Postgres Hidden Gems](http://www.craigkerstiens.com/2018/01/31/postgres-hidden-gems/)
* [My Favorite PostgreSQL Queries and Why They Matter](https://severalnines.com/blog/my-favorite-postgresql-queries-and-why-they-matter)

```
▶ \x auto
```

```sql
// A faster way to write `WHERE id IN ($1, $2, $3)`
SELECT * from users WHERE id = ANY($1::int[])
```

## postgresql.conf

Find postgresql.conf at macOS at:

```
/usr/local/var/postgres
```

## Common Error

```
psql: error: could not connect to server: could not connect to server: No such file or directory
	Is the server running locally and accepting
	connections on Unix domain socket "/tmp/.s.PGSQL.5432"?
```

You may have brew upgrade from different version and so the data directory might be incompatible.

```
The data directory was initialized by PostgreSQL version 11, which is not compatible with this version 12.1.
```

```
▶ tail -n 10 /usr/local/var/log/postgres.log
▶ brew postgresql-upgrade-database
```

## .psqlrc

```
\set PROMPT1 '%~%x%# '
\x auto
\set ON_ERROR_STOP on
\set ON_ERROR_ROLLBACK interactive

\pset null '¤'
\pset linestyle 'unicode'
\pset unicode_border_linestyle single
\pset unicode_column_linestyle single
\pset unicode_header_linestyle double
set intervalstyle to 'postgres_verbose';

\setenv LESS '-iMFXSx4R'
```

## Commands

```
select session_user;
select current_schema();
show search_path;
\dn - list all schemas
```

## Rails Related

* [Support Optimizer Hints](https://github.com/rails/rails/pull/35615)
* [Add support for annotating queries generated by ActiveRecord::Relation with SQL comments](https://github.com/rails/rails/pull/35617)

## UTF8

* [MySQL schema/migration should default to CHARSET=utf8mb4](https://github.com/rails/rails/issues/33596)

## As a Job Queue

* [What is SKIP LOCKED for in PostgreSQL 9.5?](https://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5/)
* [How we built a job queue with Postgres](https://blog.holistics.io/how-we-built-a-multi-tenant-job-queue-system-with-postgresql-ruby/)

## Constraints

* [Protect Your Data with PostgreSQL Constraints](http://nathanmlong.com/2016/01/protect-your-data-with-postgresql-constraints/)
* [Using PostgreSQL constraints](https://arjanvandergaag.nl/blog/using-postgresql-constraints.html)

## RBAC

* [Postgraphile - Postgres for GraphQL](https://www.graphile.org/postgraphile/introduction/)
* [Role-based grant system](https://www.postgresql.org/docs/9.6/static/user-manag.html)
* [RLS - Row-level security policies](https://www.postgresql.org/docs/9.6/static/ddl-rowsecurity.html)

## Rollup

* [Efficient rollup tables with HyperLogLog in Postgres](https://www.citusdata.com/blog/2017/06/30/efficient-rollup-with-hyperloglog-on-postgres/)
* [Building Real Time Analytics APIs at Scale](https://blog.algolia.com/building-real-time-analytics-apis/)

## Time-Series

* [Problems with PostgreSQL 10 for time-series data](https://blog.timescale.com/time-series-data-postgresql-10-vs-timescaledb-816ee808bac5)
* [Time Series Database Lectures - Fall 2017](http://db.cs.cmu.edu/seminar2017/)

## Indexing

* [Principles and Applications of the Index Types Supported by PostgreSQL](https://medium.com/@Alibaba_Cloud/principles-and-applications-of-the-index-types-supported-by-postgresql-481f59bab67d)
* [Introducing Dexter, the Automatic Indexer for Postgres](https://medium.com/@ankane/introducing-dexter-the-automatic-indexer-for-postgres-5f8fa8b28f27)
* [How About Hypothetical Indexes ?](https://rjuju.github.io/postgresql/2015/07/02/how-about-hypothetical-indexes.html)
* Indexes add overhead to write operations like INSERT, UPDATE and DELETE. Because of this, you may not want to index write-heavy tables.
* [Index Columns for `LIKE` in PostgreSQL](https://niallburkley.com/blog/index-columns-for-like-in-postgres/)

**BRIN**

```sql
--- table with 100M rows
CREATE TABLE test_bitmap AS
  SELECT mod(i, 100.000) AS val
    FROM generate_series(1, 100.000.000) s(i);
CREATE INDEX test_btree_idx ON test_bitmap(val);
CREATE INDEX test_brin_idx ON test_bitmap USING brin(val);
```

```sql
CREATE EXTENSION IF NOT EXISTS "btree_gist";
CREATE INDEX ON dynos USING GiST (
  instance_id, tstzrange(started_at, stopped_at, '[]')
)
CREATE INDEX ON dynos (started_at)
CREATE INDEX ON dynos (stopped_at)

SELECT instance_id, started_at, stopped_at
FROM dynos
WHERE instance_id=453345
AND tstzrange(started_at, stopped_at, '[]') @> '2017-05-08 00:57:41'::timestamptz;
```

## Indexing Problem

B-tree indexes will bloat overtime.

* Lots of updates == bloated indexes
* Indexes take up way more space on disk than necessary
* Very significant on large, busy tables
* [Use check_postgres to monitor index bloat](https://github.com/bucardo/check_postgres)
* [Check your index bloat](https://gist.github.com/jberkus/9923948)
* An index that is not updated will not bloat, so try to not index null columns. Make no sense to index million of null values.
* Updating indexes makes writes slower. Write amplification.

```
select pg_size_pretty(pg_relation_size('user_pkey'));
reindex index user_pkey;
```

Note that REINDEX is not a concurrent operation.

[Lessons Learned Operating Postgres at Scale Dave Pirotte](https://www.youtube.com/watch?v=-LCOAQZuuVo)

Only index low cardinality column. If you have 10% true and 90% false, you might as well index those 10% true only.

```
create index concurrently index_where_true on large_table (low_cardinality_column) where low_cardinality_column = 't';
```

## Scaling

* [Citus - For large Postgres](https://www.citusdata.com/)
* [Postgres-XL](http://www.postgres-xl.org/)
* [Greenplum](http://greenplum.org/)
* [cstore_fdw - Columnar store for analytics with PostgreSQL](https://github.com/citusdata/cstore_fdw)

## Embrace Constraints

* NOT NULL
* CREATE UNIQUE INDEX
* Use check constraints with PLV8??

## Performance

Most single queries should be aiming for around a 1ms query time.

* `pg_stat_statements` is a MUST
* [PGTune](https://pgtune.leopard.in.ua)
* [Expensive Query Dashboard](https://blog.heroku.com/expensive-query-speed-up-app)
* [Using Rack Mini Profiler to find and fix slow queries](https://schneems.com/2017/06/22/a-tale-of-slow-pagination/)
* pg_buffercache extension to see how much shared_buffers to allocate
* work_mem - memory allocated to do sort operation
* [PostgresOpen 2019 Mistaken And Ignored Parameters While Optimizing A PostgreSQL Database](https://www.youtube.com/watch?v=lJ18c1hGRBM)
* Alway on autovacuum and track_count
* Know your disk IOPS. Default settings in Postgres is for long spinning disks. Now we have faster SSD.
* random_page_cost=4 with SSD vs seq_page_cost=1. If random_page_cost is high, query plan may prefer the low cost seq_page_cost.
* wal_compression

```
SELECT substring(query, 1, 50) AS short_query,
  round(total_time::numeric, 2) AS total_time,
  calls, round(mean_time::numeric, 2) AS mean,
  round((100 * total_time / sum(total_time::numeric) OVER ())::numeric, 2) AS percentage_overall
FROM pg_stat_statements
ORDER BY total_time DESC
LIMIT 20;
```

Allow you to see potential missing indexes:

```
SELECT schemaname, relname, seq_scan, seq_tup_read,
  idx_scan, seq_tup_read / seq_scan AS avg
FROM pg_stat_user_tables
WHERE seq_scan > 0
ORDER BY seq_tup_read DESC;
```

## Connection Pools

If you need more than 500 connections, use PGBouncer.

* [Managed Databases Connection Pools and PostgreSQL Benchmarking Using pgbench](https://www.digitalocean.com/community/tutorials/managed-databases-connection-pools-and-postgresql-benchmarking-using-pgbench)
* [Scalable PostgreSQL connection pooler](https://github.com/yandex/odyssey)
* [How to Pool PostgreSQL Connections with PgBouncer](https://www.compose.com/articles/how-to-pool-postgresql-connections-with-pgbouncer/)

## UUID

Random UUID can cause WAL write amplification?

Sequential UUID generation can avoid fragmentation issue.

* [A Brief History of the UUID](https://segment.com/blog/a-brief-history-of-the-uuid/)
* [UUID or GUID as Primary Keys? Be Careful!](https://tomharrisonjr.com/uuid-or-guid-as-primary-keys-be-careful-7b2aa3dcb439)
* [The Cost of GUIDs as Primary Keys](http://www.informit.com/articles/article.aspx?p=25862)
* [Why Auto Increment Is A Terrible Idea](https://www.clever-cloud.com/blog/engineering/2015/05/20/why-auto-increment-is-a-terrible-idea/)
* [Sequential UUID Generators](https://www.2ndquadrant.com/en/blog/sequential-uuid-generators/)
* [Storing UUID Values in MySQL](https://www.percona.com/blog/2014/12/19/store-uuid-optimized-way/)

To use UUID as primary key, it must be sortable. Sorting by its raw bytes results in a sequence equivalent to sorting by its embedded timestamp, thus making it naturally more useful as primary key and providing improved reference locality and thus insert performance. Primary keys that are out of sequence are not just useless for sorting, but can also cause unnecessary disk access due to occupying significantly different locations in the index.

UUID v6 may solve this?

## EXPLAIN and EXPLAIN ANALYZE

* [Explaining the unexplainable](https://www.depesz.com/2013/05/09/explaining-the-unexplainable-part-3)
* [explain.depesz.com - Postgres explain analyze made readable](https://explain.depesz.com/)
* [Distinguishing Access and Filter-Predicates](http://use-the-index-luke.com/sql/explain-plan/postgresql/filter-predicates)
* [Postgres EXPLAIN Visualizer (Pev)](http://tatiyants.com/pev/#/plans/new)

Use parallel queries: split queries to multiple CPU using `max_parallel_workers` and `max_parallel_workers_per_gather` and `dynamic_shared_memory_type`

## Lateral Join

* [Postgres powerful new join type: Lateral](https://blog.heapanalytics.com/postgresqls-powerful-new-join-type-lateral/)
* [PostgreSQL's LATERAL JOIN](https://medium.com/kkempin/postgresqls-lateral-join-bfd6bd0199df)

## SKIP LOCKED

* [PG Casts - The Skip Locked feature in Postgres 9.5](https://www.pgcasts.com/episodes/7/skip-locked/)
* Good for implementing queue?

## pg_tune

## Incidents to learn

* [Gitlab DB incident](https://about.gitlab.com/2017/02/01/gitlab-dot-com-database-incident/)

## Materialized/SQL Views

Introducing views will enrich your domain vocabulary and help your application deal more with *business logic* than with *storage logic*.

* [Speed up with Materialized Views on PostgreSQL and Rails](https://www.sitepoint.com/speed-up-with-materialized-views-on-postgresql-and-rails/)
* Essentially a caching system at the database
* Virtual table
* Providing roll-ups

## Window Functions

A window function call represents the application of an aggregate-like function over some portion of the rows selected by a query.

Useful for doing "running aggregate/total".

See https://www.youtube.com/results?search_query=bruce+momjian+window+functions

Unlike aggregate function like `COUNT()`, `SUM()`, `AVG()`, which operate on an entire table, Window Functions operate on a set of rows and return a single aggregated value for each row.

The rows retain their separate identities and the aggregated value is merely added to each row rather than grouped together.

* [Folding Postgres Window Functions into Rails](https://blog.codeship.com/folding-postgres-window-functions-into-rails/)
* [Modeling Postgres Common Table Expressions and Window Functions with Rails and ActiveRecord](http://blog.nrowegt.com/modeling-postgres-common-table-expressions-and-window-functions-with-rails-and-activerecord/)
* [Advanced SQL - window functions](http://mjk.space/advanced-sql-window-functions/)
* ANSI/ISO Standard SQL:2003
* ANSI/ISO Standard SQL:2008
* Microsoft SQL Server 2005 implemented (ROW_NUMBER, RANK, NTILE and DENSE_RANK functions only)
* SQL Server 2012 implemented the full range of functions finally
* MariaDB 10.2.0 implemented Window Functions finally
* MySQL 8 implemented Window Functions finally

The strength of window functions is not pagination, but **analytical calculation**.

The set of rows used for the calculation is called the "window" and will the entire dataset by default.

The `PARTITION BY` is employed to reduce the window to a particular group within the dataset.

## Partition

* [pg_partman](https://github.com/keithf4/pg_partman)
* [partitionable](https://github.com/pacuna/partitionable)
* [Video: RailsConf 2019 - Postgres & Rails 6 Multi-DB: Pitfalls, Patterns, Performance by Gabe Enslein](https://www.youtube.com/watch?v=a4OBp6edNaM)

Default in Postgresql 11

```
ALTER TABLE invoices ATTACH PARTITION invoices_y2018m07 FOR VALUES FROM
('2018-07-01') TO ('2018-08-01');
```

## Range Type or Temporal Extension

Using "contains @>".

"How many movies did we rented out on June 21st?"

```sql
select count(*) from rental
where rental_period @> '2019-06-21'::timestamp;
```

Using "within &&"

"How many movies did we rent during the month of June?"

```sql
select count(*) from rental
where rental_period && tsrange('2019-06-01'::timestamp, '2019-06-30'::timestamp);
```

## Date and Time

* [Dealing With User Timezones in Postgres](http://idlehands.codes/dealing-with-timezones-in-postgres)
* [Dealing With Time Zones Using Rails and Postgres](http://brendankemp.com/essays/dealing-with-time-zones-using-rails-and-postgres/)
* [How to deal with timestamps?](https://www.depesz.com/2014/04/04/how-to-deal-with-timestamps/)
* [Working With Time in Postgres](http://www.craigkerstiens.com/2017/06/08/working-with-time-in-postgres/)
* [Better Date Manipulation in PostgreSQL Queries](https://robots.thoughtbot.com/better-date-manipulation-in-postgres-queries)

```sql
WHERE inserted_at
BETWEEN ('2016-12-21 00:00:00.000'::timestamp AT TIME ZONE u.timezone)
AND
        ('2016-12-21 23:59:59.999'::timestamp AT TIME ZONE u.timezone)
```

```sql
AVG(sale.price) as average_price,
MAX(sale.price) as max_price,
MIN(sale.price) as min_price,
ROUND(AVG(EXTRACT(EPOCH FROM sale.completed_at) - EXTRACT(EPOCH FROM sale.created_at))) as average_sale_time
```

## Array

* [Postgres, the Good Parts: Arrays](http://blog.ryankelly.us/2016/08/21/postgres-the-good-parts-arrays.html)

## JSONB

* 12 operators and 23 functions (as of 9.5)
* 2 different indexing methods
* Good for storing, but need to be replaced entirely for updates
* [How to convert json string to text?](https://stackoverflow.com/questions/27215216/postgres-how-to-convert-json-string-to-text/31757242#31757242)

## Range

* tstzrange

```sql
CREATE TABLE billings (
  formation_id    uuid      NOT NULL,
  validity_period tstzrange NOT NULL,
  price_per_month integer   NOT NULL
);

# Exclusion constraints
ALTER TABLE ONLY billings
ADD CONSTRAINT billings_excl
EXCLUDE USING gist (
  uuid_send(formation_id) WITH =,
  validity_period WITH &&
)
```

## ENUM

* Good for small unchanging constants like AWS regions

## Pagination - OFFSET vs Cursor/Keyset

> offset pagination is broken and there are other ways to paginate

* [Five ways to paginate in Postgres, from the basic to the exotic](https://www.citusdata.com/blog/2016/03/30/five-ways-to-paginate/)
* [Build JSON API Responses With Postgres CTEs](http://www.codedependant.net/2017/04/30/build-json-api-responses-postregres-with-cte/)
* [We need tool support for keyset pagination](http://use-the-index-luke.com/no-offset)
* [Pagination: You're (Probably) Doing It Wrong](https://coderwall.com/p/lkcaag/pagination-you-re-probably-doing-it-wrong)
* [We need tool support for keyset pagination](https://use-the-index-luke.com/no-offset)

## PL/V8

* [JavaScript in your Postgres](https://blog.heroku.com/javascript_in_your_postgres)

## RAM

## Parallel Query

* For long query only, like seconds. If your queries finished quick in millisecond, then there is no point to do parallel query.
* Not for more concurrent queries than CPU.
* Does not make your disk spin any faster. Not for IO bound queries also.
* 9.6 has Parallel Sequential Scan
* Parallel aggregate and parallel join
* [Parallel Query In PostgreSQL Robert Haas](https://www.youtube.com/watch?v=pjEySZAv9Kw)
* Best for 4 workers?
* Good for JOIN partner? SUM?

## Full-Text Search

* [Postgres Text Search: Simple, Adequate](http://info.pagnis.in/blog/2017/06/18/postgres-text-search-simple-adequate/)
* [Implementing Multi-Table Full Text Search with Postgres in Rails](https://robots.thoughtbot.com/implementing-multi-table-full-text-search-with-postgres)

## Backup

* pg_dump is good for logical backup with less than 100GB of data
* Once you reached data that can't fit into RAM, you need to use physical backup like [WAL-E](https://github.com/wal-e/wal-e)
* [3 Methods of backing up Postgres](https://www.urbackup.org/backup_postgresql.html)
* [WAL-E - Continuous archiving for Postgres](https://github.com/wal-e/wal-e)
* [pg_backrest](https://pgbackrest.org/)

## Migration

* [Continuous migration with pgloader](https://pgloader.io/blog/continuous-migration/)

## Compression

* TOAST - Automatic Table Compression

## Auto-vacuum

* Cleans up dead tuples, reclaiming space for Postgres (not for OS)
* Auto-vacuum can cause outages.
* [PostgreSQL Bloatbusters](http://blog.dataegret.com/2018/03/postgresql-bloatbusters.html)

## Monitoring

* [Key metrics for PostgreSQL monitoring](https://www.datadoghq.com/blog/postgresql-monitoring/)

## People

* [Craig Kerstiens](http://www.craigkerstiens.com/)

## Blog

* [PG Analyze blog](https://pganalyze.com/blog)
* [](https://brandur.org/articles)

## Videos

* [PostgresOpen](https://www.youtube.com/user/postgresopen)
* [dotScale 2017 - Marco Slot - Scaling out (Postgres)SQL](https://www.youtube.com/watch?v=xJghcPs0ibQ)
* [Skillsmatter's Postgres Videos](https://skillsmatter.com/explore?q=tag%3Apostgresql)
* [PostgresOpen 2019 The Art Of PostgreSQL](https://www.youtube.com/watch?v=q9IXCdy_mtY)
* [Postgres Open Conference](https://www.youtube.com/user/postgresopen/videos)
* [Bruce Momjian window functions](https://www.youtube.com/results?search_query=bruce+momjian+window+functions)

